# -*- coding: utf-8 -*-
"""[Weekly_Project]_Boston_House_Pricing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRhpe35Ja3KrzJTglmAE4R-1orPflb1E

![](https://media.makeameme.org/created/i-can-sell-596e8a.jpg)

# Weekly Project: Predicting Boston Housing prices

In this project, you will create, evaluate the performance and predictive power of your model on data collected from homes in suburbs of Boston, Massachusetts

The dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the **506 entries** represent aggregated data about **14 features** for homes from various suburbs in Boston, Massachusetts. 

**Data preprocessing has been applied to your dataset**. For the purposes of this project, the following preprocessing steps have been made to the dataset:

- 16 data points have an 'MEDV' value of 50.0. These data points likely contain missing or censored values and have been removed.
- 1 data point has an 'RM' value of 8.78. This data point can be considered an outlier and has been removed.
- The features 'RM', 'LSTAT', 'PTRATIO', and 'MEDV' are essential. The remaining non-relevant features have been excluded.
- The feature 'MEDV' has been multiplicatively scaled to account for 35 years of market inflation.

🎯 YOUR TASK: apply a machine learning model of your choice to **predict the price** of houses in a Boston neighborhood (which is column '**MEDV**')

The dataset can be accessed here: https://raw.githubusercontent.com/anhquan0412/boston_housing/master/housing.csv

Here are the few steps you can follow
"""

import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/anhquan0412/boston_housing/master/housing.csv')
df.head()

"""

# **Step 1: Data Exploration (EDA)**

Familiarizing yourself with the data through an explorative process is a fundamental practice to help you better understand and justify your results.

As a reminder, we are using three features from the Boston housing dataset: 'RM', 'LSTAT', and 'PTRATIO'. For each data point (neighborhood):

- '**RM**' is the **average number of rooms** among homes in the neighborhood.
- '**LSTAT**' is the **percentage** of homeowners in the neighborhood considered "**lower class**" (working poor).
- '**PTRATIO**' is the **ratio of students to teachers** in primary and secondary schools in the neighborhood.

**Question**: For each of the three features above, do you think that an increase in the value of that feature would lead to an increase in the value of 'MEDV' or a decrease in the value of 'MEDV'? Justify your answer for each by plotting or calculating the correlation

"""

import numpy as np
import pandas as pd
import seaborn as sns

df.info()

df.describe()

df.corr()

# Trong khoảng (0,4.5) RM tăng MEDV giảm và ngược lại khi RM > 4.5, tỷ lệ thuận
sns.scatterplot(data=df, x= 'RM', y= 'MEDV')

# Nhìn chung LSTAT tăng thì MEDV giảm, tỷ lệ nghịch
sns.scatterplot(data=df, x= 'LSTAT', y= 'MEDV')

# Nhìn tổng quát,PTRATIO có xu hướng tỷ lệ nghịch với MEDV nhưng không quá rõ ràng, correlation thấp: -0.52
sns.lineplot(data=df, x= 'PTRATIO', y= 'MEDV')

"""# **Step 2: Get to know our performance Metric**

Do a quick read on Mean Absolute Error (MAE), because that is going to be the metric we are going to use for this task.

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html

**Question**: Calculate MAE on these data points by using the Sklearn library
```python
y_true = [3, -0.5, 2, 7, 4.2]
y_pred = [2.5, 0, 2.1, 7.8, 5.3]
```
If a model is making these prediction (y_pred), is this a good model?
"""

# YOUR CODE HERE
from sklearn.metrics import mean_absolute_error
y_true = [3, -0.5, 2, 7, 4.2]
y_pred = [2.5, 0, 2.1, 7.8, 5.3]

mean_absolute_error(y_true, y_pred)

"""Very good model

# **Step 3: Train our machine learning model**

Your task is to build a pipeline to preprocess this data and train a linear regression model to predict the house price. Make sure to follow all the recommended practices we mention in the class. 

Here are 2 pipelines you should implement:
- The **1st pipeline** include: MinMaxScaler (or Standard Scaler) as **Transformer** (to preprocess), and Linear Regression as **Estimator** (machine learning model)

- The **2nd pipeline** include: Polynomial Feature, **then** Standard Scaler as **Transformers**, and Linear Regression as **Estimator**. Pay attention that you have 2 transformers in a very specific order.
"""

X = df[['RM','LSTAT','PTRATIO']]
y = df['MEDV']

from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# Define your 1st pipeline here
pipe1 = Pipeline([('scaler', StandardScaler()),
                  ('model', LinearRegression())])

# Define your 2nd pipeline here
pipe2 = Pipeline([('poly',PolynomialFeatures(degree = 1, include_bias=False)),
                  ('scaler', StandardScaler()),
                  ('model', LinearRegression())])

"""## 3.1 Use a simple train-test split

*Do these following step for the first pipeline only*:

- Use sklearn `train_test_split` to split data into train set and test set. Use random_state = 42
- Fit the pipeline on train set
- Use pipline to predict the test set
- Calculate the MAE on test set
"""

# YOUR CODE HERE
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

pipe1.fit(X_train, y_train)
y_test_pred = pipe1.predict(X_test)

# MAE on test set
mean_absolute_error(y_test, y_test_pred)



"""## 3.2a: Use 5 fold cross-validation *without* polynomial features

*Do these following steps for first pipeline*

You don't do a simple train-test split. Instead, you will **do train-test split 5 times** and then calculate the mean of the MAE score for these 5 test set.

Take a look at K-fold cross validation section in the sklearn tutorial or  [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) and write a code to **calculate the mean of MAE of these 5-fold cross validation results**



*Hint*: You need to read the documentation to know exactly what will be returned when you use `cross_val_score(your_pipeline, X,y, scoring="neg_mean_absolute_error")`. And note that the only scoring that similar to MAE in `cross_val_score` is `neg_mean_absolute_error`, which means the result will be negative MAE. All you need to do is *multiply the result by -1* to turn them into positive MAE.
"""

# YOUR CODE HERE
from sklearn.model_selection import cross_val_score

score= (cross_val_score(pipe1, X, y, scoring="neg_mean_absolute_error"))*-1

# Mean MAE 
score.mean()

"""## 3.2b: Use 5 fold cross-validation *with* polynomial features

*Do these following steps for first pipeline*

Repeat everything in **Part 2** on the second pipeline (The one with Polynomial Feature as one of the transformer). For this one, write a `for` loop to **test out different degrees** for Polynomial Feature. Here is roughly what your code should look like
```
for each value n in a range of your choice:
    - create a pipeline including Polynomial Feature(degree=n) => MinMaxScaler => Linear Regression
    - call cross_val_score on this pipeline
    - calculate mean of MAE scores from cross validation
```
"""

# YOUR CODE HERE
for n in np.arange(1,10):
  pipe = Pipeline([('poly',PolynomialFeatures(degree = n, include_bias=False)),
                  ('scaler', StandardScaler()),
                  ('model', LinearRegression())])
  score= (cross_val_score(pipe, X, y, scoring="neg_mean_absolute_error"))*-1
  mean_MAE = score.mean()
  print('degree:', n)
  print(mean_MAE)

"""## 3.3c: Use GridSearchCV to find the best option

Now, let's upgrade the code using GridSearchCV. Please refer to the Sklearn tutorial and [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for details
"""

# YOUR CODE HERE
from sklearn.model_selection import KFold
kfold_shuffle = KFold(n_splits = 18,shuffle=True,random_state=42)

param_grid = {'poly__degree': [1,2,3,4,5,6,7,8,9,10]}

from sklearn.model_selection import GridSearchCV
GSC = GridSearchCV(pipe2,
                   param_grid=param_grid,
                   scoring='neg_mean_absolute_error',
                   cv=kfold_shuffle,
                   n_jobs=-1,
                   verbose=True)

GSC.fit(X_train,y_train)

pd.DataFrame(GSC.cv_results_)[pd.DataFrame(GSC.cv_results_)['rank_test_score'] == 1]

"""# **Step 4: Report your MAE**

From your results above, which pipeline configuration ends up with the lowest mean MAE from cross validation? And what is that MAE?

ANSWER HERE: 

1.   Pipeline:
*   pipeline = Pipeline([('poly', PolynomialFeatures(degree = 3,include_bias=False)),
('scaler', StandardScaler()),('model', LinearRegression())])
2.   Lowest mean MAE: 54607
"""

best_estimator = GSC.best_estimator_

best_estimator

best_estimator.fit(X_train,y_train)

y_test_pred = best_estimator.predict(X_test)

# MAE on train test
y_train_pred = best_estimator.predict(X_train)
print(mean_absolute_error(y_train,y_train_pred))

# MAE on test set
print(mean_absolute_error(y_test,y_test_pred))

pd.DataFrame(y_test).tail()

pd.DataFrame(y_test_pred).tail()

"""# **Step 5: Make predictions**

Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell. You have collected the following information from three of your clients:

|Feature	|Client 1	|Client 2	|Client 3|
|-----------|-----------|-----------|--------|
|Total number of rooms in home	|5 rooms	|4 rooms	|8 rooms|
|Neighborhood poverty level (as %)	|17%	|32%	|3%|
|Student-teacher ratio of nearby schools	|15-to-1	|22-to-1	|12-to-1|

Based on your model, what price would you recommend each client sell his/her home at?

Note that at this point, you haven't had a trained model to make the prediction on this data. So we will create a pipeline and train it.

To do this question, you need to
- Redefine the best pipeline that you report in step 4
- **Fit** this pipeline on the entire dataset given at the beginning (we don't need to do train/test split here as we already know this is our best pipeline)
- Manually create a new test data including 3 clients from the table above
- Use the pipeline you have fitted to **predict** this new test data
- Save the prediction to a variable called `client_preds`
"""

# YOUR CODE HERE
estimator = LinearRegression()
pipeline = Pipeline([('poly', PolynomialFeatures(degree = 3,include_bias=False)),
                    ('scaler', StandardScaler()),('model', estimator)])

new_test = pd.DataFrame([[5,17,15],[4,32,22],[8,3,12]])

new_test

pipeline.fit(X,y)

client_preds = pipeline.predict(new_test)

client_preds

"""After you have your variable `client_preds`, run this code to visualize your predictions against the home prices (MEDV)

```python
import matplotlib.pyplot as plt
for i,price in enumerate(client_preds):
    plt.hist(y, bins = 30,color='r',ec='black')
    plt.axvline(price, lw = 3)
    plt.text(price-50000, 40, 'Client '+str(i+1), rotation=90)
```

Note that 
- `client_preds` should be a vector of size 3 (since we only have 3 records in the new test set) 
- `y` should be the column MEDV in the dataset you use to train
"""

# YOUR CODE HERE
import matplotlib.pyplot as plt
for i,price in enumerate(client_preds):
    plt.hist(y, bins = 30,color='r',ec='black')
    plt.axvline(price, lw = 3)
    plt.text(price-50000, 40, 'Client '+str(i+1), rotation=90)

df[df['MEDV'] > 1000000]

new_test.columns=('RM','LSTAT','PTRATIO')
new_test

"""Do these prices seem reasonable given the values for the respective features?

#### Đúng
Client 1 và 2 nằm trong range của MEDV nên không có gì bất thường;
Client 3 nằm ngoài range, lớn hơn max rất ít nhưng vì so với MEDV của houses trên 1.000.000 thì 2 features RM và PTRATIO thấp hơn 1 chút kéo giá xuống nhưng LSTAT của Client 3 thấp hơn khiến giá cao hơn.

# **Step 6: Applicability**

In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.

Hint: Take a look at the range in prices as calculated in the code snippet above. Some questions to answering:

1. How relevant today is data that was collected from 1978? How important is inflation?
  - Answer: Rất quan trọng vì range ko đổi, nhưng value sẽ thay đổi dựa vào lạm phát vì thập niên 1970 lạm phát tăng
2. Are the features present in the data sufficient to describe a home? Do you think factors like quality of apppliances in the home, square feet of the plot area, presence of pool or not etc should factor in?
  - Answer: Chưa đủ để đánh giá giá nhà vì còn nhiều yếu tố như tiện nghi, diện tích, location, nhà cũ hay mới, chủ đầu tư,....
3. Is the model robust enough to make consistent predictions?
Would data collected in an urban city like Boston be applicable in a rural city?
  - Answer: Không, bởi vì ở nông thôn cách nghĩ khác với dân thành thị
4. Is it fair to judge the price of an individual home based on the characteristics of the entire neighborhood?
  - Answer:Không bởi vì phải đánh giá dựa trên cả yếu tố bên trong ngôi nhà,....
"""

